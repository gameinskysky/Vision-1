\chapter{Methods}
\label{sec:methods}

The main goal of all localization systems is the estimation of the robot pose, which describes the robots position and orientation in space. All visual-inertial localization approaches have in common, to augment the pose estimation with the estimation of IMU characteristics and landmark positions. The methods to achieve this estimation differ in the different implementations. Sections \ref{sec:rovio} and \ref{sec:okvis} will introduce the working principles, characteristics, and differences of ROVIO and OKVIS, respectively. In section \ref{sec:metrics} the evaluation metrics are introduced, which will be used throughout this report to describe the performance of a visual-inertial localization algorithm. We propose a distinction between global and local accuracy to get deeper insights into the differences and issues within the two implementations. 

\section{ROVIO}
\label{sec:rovio}

ROVIO stands for robust visual-inertial odometry and it is based on an extended kalman filter (EKF) for estimation. The filter state contains the robot-centric position, velocity and attitude, the accelerometer and gyroscope biases, the extrinsics between camera and IMU, and the landmarks in a robot-centric representation. Every landmark within the filter state is described with a bearing-vector and the distance between the camera frame and the landmark. The estimation is performed in two steps, leading to a tight-integration of the IMU and camera measurements. Firstly, the filter state is propagated between two frames based on the IMU measurements. Secondly, as soon as new frame is available, the filters prior belief is updated based on the camera measurement. ROVIO is detecting new landmarks with a FAST corner detector (Rosten et al., \cite{rosten2006machine}) and uses multilevel direct intensity patch features for landmark representation. Direct image intensity patches with a size of 8x8 pixels are extracted on different pyramid levels. If a new image is processed, the direct image patches of the predicted features are warped and compared with the new observation to calculate a direct intensity error. This error is used for the innovation term to update the filter state. Figure \ref{pics:rovio_frontend} visualizes ROVIO during operation and gives an impression about the features that are extracted.

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{images/rovio_frontend.png}
   \caption{ROVIO visualization during operation: On the left side the input image with the extracted and reprojected landmarks is shown, on the right side the fixed inertial frame (large frame), the estimated body frame (small frame) and the landmarks (white dots) with uncertainty (green lines) are visualized.}
   \label{pics:rovio_frontend}
\end{figure}

The computational complexity of an EKF increases with $O(n^3)$, where $n$ is the number of filter states, or also, in our case, the number of landmarks (Strasdat et al., \cite{strasdat2010real}). This cubic dependence is the reason, why ROVIO is working with a limited and small number of landmarks, typically between 10 and 50. To get high quality features, ROVIO has a landmark management system where features are added and removed according to a quality score based on their gradient matrix, their distance to other features and their tracking quality over the duration they have been in the filter state. 

A key assumption within each EKF approach is a gaussian distribution for both the process noise and the measurement noise. The EKF performs a linearization at every update step around the filters prior belief. This step introduces linearization errors. These errors and the filtering-based property of not correcting past estimates based on new observations (see section \ref{sec:introduction}) are the main reasons why ROVIO is prone to drift, as we will see in section \ref{sec:results}.



\section{OKVIS}
\label{sec:okvis}

OKVIS stands for optimal keyframe-based visual-inertial SLAM and achieves the localization also by tight-integration of IMU and camera measurements. OKVIS performs a nonlinear optimization to estimate the robot pose. The unknowns are given by the robot position and attitude for a set of past keyframes and the newest frame on one side, and by the robot velocity and IMU biases for the last few temporal frames on the other side. The constraints are given by the IMU measurements and the landmark observations taken by the camera. IMU error terms and reprojection error terms are combined into a single error function. OKVIS detects new features based on a multi-scale Harris corner detector (Harris et al., \cite{harris1988combined}) and describes them with a BRISK descriptor (Leutenegger et al., \cite{leutenegger2011brisk}). In order not to grow unbounded, a marginalization is performed to get rid off old keyframes and temporal frames but still keeping as much as possible of useful information within the framework. Figure \ref{pics:rovio_frontend} visualizes OKVIS during operation and gives an impression on the extracted features within the framework.

\begin{figure}[h]
   \centering
   \includegraphics[width=0.6\textwidth]{images/okvis_frontend.png}
   \caption{OKVIS visualization during operation: For the latest keyframe (top) and the current frame (bottom) the extracted features are visualized with circles and the green lines denote matching pairs. Red circles stand for unmatched features, blue circles for features matched to another keyframe.}
   \label{pics:okvis_frontend}
\end{figure}

The computational complexity of bundle adjustment methods scale with $O(m^2*n)$, where $m$ is the number of keyframes and $n$ is the number of landmark observations per frame (Strasdat et al., \cite{strasdat2010real}). The number of keyframes is typically small (below 15) and the number of landmarks per frame for OKVIS is typically between 50 and 250. OKVIS is therefore executing a nonlinear optimization over a number of landmarks in the order of 1000. 

Working with more information, omitting linearization of the reprojection terms and the keyframe-based property of allowing poses of the past to be corrected based on a new observation, make OKVIS less prone to drift as we will see in section \ref{sec:results}. This advantage comes at the expense of a higher computational complexity.



\section{Evaluation Metrics}
\label{sec:metrics}

To compare the presented algorithms, the errors between estimated pose and a ground truth trajectory will be analysed. The ground truth is given by an external vicon measurement system providing accurate 6D pose estimates. Working with a simplified model, we introduce an inertial coordinate frame $I$, which is world-fixed and a body coordinate frame $B$, which is rigidly connected to the robot. 

The position of the robot with respect to $I$ is given by $_I \vec{r}_{IB} = \left[ \begin{array}{ccc} x& y& z \end{array} \right]^T$ and describes the vector from the origin of $I$ to the origin of $B$ in Cartesian coordinates. 

The attitude of the robot is usually parametrized either by a rotation matrix, a triple of Euler angles, or by a unit quaternion. We will mainly use rotation matrices and, at some point, Euler angles following the Tait-Bryan convention (roll pitch yaw). Following Diebel et al. \cite{diebel2006representing}, the rotation matrix $R$, describing the attitude of the robot and hence encoding the orientation of the body frame with respect to the inertial frame, is given by 

\begin{equation}
       R = \left[ \begin{array}{ccc}
		r_{11} & r_{12}	& r_{13} \\
		r_{21} & r_{22} 	& r_{23} \\
		r_{31} & r_{32}	& r_{33} \\
		\end{array} \right]
\end{equation}

and maps a vector $_I \vec{a} $, described in world coordinates, to the same vector $_B \vec{a} $, described in body coordinates,

\begin{equation}
	_B \vec{a} = R * _I \vec{a} .
\end{equation}


The robot pose, given by $ \left( \begin{array}{cc} _I \vec{r}_{IB}, & R \end{array} \right) $, is a function of time. We introduce the discrete time index $n$ and we define the robot pose at time index $n$ finally by 

\begin{equation}
	T(n) = \left( \begin{array}{cc} _I \vec{r}_{IB}(n), & R(n) \end{array} \right)
\end{equation}

We assume that the vicon measurement is exact and therefore supplies directly the true pose $T(n)$. To highlight an estimate of an expression we use the hat-notation. Doing so, the estimated pose is denoted by

\begin{equation}
	\hat{T}(n) = \left( \begin{array}{cc} _I \hat{\vec{r}}_{IB}(n), & \hat{R}(n) \end{array} \right) .
\end{equation}


\subsection{Global accuracy}
\label{sec:global}

Looking at the performance of a localization algorithm, both an accurate global and an accurate local pose estimation are important. The global accuracy is important for tasks like autonomous operation without GPS availability or longer indoor operation. To describe global accuracy the true and estimated trajectories have to be aligned for time $n=0$ (see figure \ref{pics:absolute_relative} a) and the absolute translation error $ATE$ and absolute orientation error $AOE$ at time $n$ can simply be defined as

\begin{equation}
       ATE \left( n \right) = \left\Vert\left(\ \hat{\vec{r}} \left( n \right) - \vec{r} \left( n \right) \right) \right\Vert_2
\end{equation}
\begin{equation}
       AOE \left( n \right) = \arccos \left( \hat{R} *\vec{e_{x}} \cdot\left( R *\vec{e_{x}} \right)\right) ,
\end{equation}

where $\left\Vert \dots \right\Vert_2$ denotes the 2-norm, $\vec{e_{x}} = \left[ \begin{array}{ccc} 1& 0& 0 \end{array} \right]^T$ denotes the unit vector in $x$ direction and $\cdot$ indicates the dot-product. At some point it will be useful to look into the 6 degrees of freedom separately and we define the absolute errors of the separated axes by

\begin{equation}
       E_{x} \left( n \right) = \left| \hat{x} \left( n \right) - x \left( n \right) \right| 
\end{equation}
\begin{equation}
       E_{y} \left( n \right) = \left| \hat{y} \left( n \right) - y \left( n \right) \right|
\end{equation}
\begin{equation}
       E_{z} \left( n \right) = \left| \hat{z} \left( n \right) - z \left( n \right) \right|
\end{equation}

\begin{equation}
	E_{\phi} \left( n \right) = \left| \hat{\phi} \left( n \right) - \phi \left( n \right)\right|
\end{equation}
\begin{equation}
	E_{\theta} \left( n \right) = \left| \hat{\theta} \left( n \right) - \theta \left( n \right)\right|
\end{equation}
\begin{equation}
	E_{\psi} \left( n \right) = \left| \hat{\psi} \left( n \right) - \psi \left( n \right)\right| ,
\end{equation}

where $\phi$, $\theta$ and $\psi$ denote the roll, pitch and yaw angle, respectively and $\left| \dots \right|$ stands for the absolute value.


\begin{figure}
  \hspace*{1cm}
  \begin{subfigure}[b]{0.3\textwidth}
    \captionsetup{skip=6pt}
    \includegraphics[width=\textwidth]{images/ape_frames_2.png}
    \caption{global accuracy}
    \label{fig:3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.28\textwidth}
    \captionsetup{skip=6pt}
    \includegraphics[width=\textwidth]{images/rpe_frames_2.png}
    \caption{local accuracy}
    \label{fig:4}
  \end{subfigure}
  \hspace*{1cm}
\caption{(a) visualizes the global accuracy: The trajectories are aligned at the first frame $F_{0}$ and the absolute difference between ground truth and estimation at time $n$ defines the global accuracy. (b) visualizes the local accuracy: The relative changes between two frames $F_i$ and $F_{i+\Delta i}$ given by the ground truth and the estimation are compared.}
\label{pics:absolute_relative}
\end{figure}





\subsection{Local accuracy}
\label{sec:local}

The local accuracy is described by relative differences between ground truth and estimation, see figure \ref{pics:absolute_relative} b. Local accuracy is important for tasks like dense reconstruction of a surface or a direct manipulation the robot wants to achieve. 

To describe the relative translation error we compare the estimated with the true travelled distance between two frames. It is a function of the frame $F_i$ at time index $i$, which we pick randomly out of the sequence and the desired distance $D_{des}$ which for our case is typically between $5cm$ and $1m$ to get interesting information about the short term accuracy of the algorithm. The relative translation error $RTE$ is defined as

\begin{equation}
RTE \left( F_i, D_{des} \right) = \left| \hat{D} \left( F_i, F_{i+\Delta i} \right) - D \left( F_i, F_{i+\Delta i} \right)\right|
\end{equation}

with $ \Delta i $ such that $ D \left( F_i, F_{i+\Delta i} \right) \geq D_{des} $. The true and estimated travelled distances can be derived by simple integration of the incremental travelled distances between frames,

\begin{equation}
       D \left( F_i, F_{i+\Delta i} \right) = \sum_{k=i}^{i+\Delta i-1} \left\Vert\left(\ r \left( k+1 \right) - r \left( k \right) \right) \right\Vert_2
\end{equation}
\begin{equation}
       \hat{D} \left( F_i, F_{i+\Delta i} \right) = \sum_{k=i}^{i+\Delta i-1} \left\Vert\left(\ \hat{r} \left( k+1 \right) - \hat{r} \left( k \right) \right) \right\Vert_2 .
\end{equation}

To describe the local accuracy in terms of orientation, the true and estimated change in attitude are compared. The relative orientation error $ROE$ is also a function of $F_i$ and $D_{des}$ and can be defined by

\begin{equation}
ROE\left( F_i, D_{des} \right) = \arccos \left(\left( \hat{R} \left(i\right)^T * \hat{R} \left(i+\Delta i\right) * \vec{e_{x}} \right) \cdot \left( R \left(i\right)^T * R \left(i+\Delta i\right) * \vec{e_{x}} \right)\right)
\end{equation}

To get a statistically meaningful result on local accuracy we will pick a number of 1000 frames $F_i$ randomly out of the sequence for a desired travelled distance $D_{des}$ and analyse the error characteristics (median, 25 and 75 percentils, outliers) of these 1000 samples.




\subsection{Computational complexity}
\label{sec:complexity}

The third important performance measure is the computational complexity of an algorithm. Often there is a trade-off between accuracy and computational complexity and depending on the application one or the other is more important.

The computational complexity can be measured by different metrics. The most universal metric would be the number of operations included in the processing of a new frame. This measure would be independent of the hardware used but it is hard to measure. Therefore it is quite common to show computational complexity with either the CPU usage or the processing time per frame on a test system. These measures are dependent on the test system but easier to measure and give an indication on the computational complexity of an algorithm. The experiments for this report have been done on a 2.5GHz Intel Core 5i macbook and we will denote computational complexity with the average processing time per frame.











